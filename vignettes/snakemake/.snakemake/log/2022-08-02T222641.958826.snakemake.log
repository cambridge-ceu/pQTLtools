Building DAG of jobs...
Job stats:
job             count    min threads    max threads
------------  -------  -------------  -------------
aggregate_MR        2              1              1
all                 1              1              1
total               3              1              1


[Tue Aug  2 22:26:42 2022]
rule aggregate_MR:
    input: output/MR/CD40_HF.csv, output/MR/CD40_HF2.csv, output/MR/OPG_HF.csv, output/MR/OPG_HF2.csv
    output: output/MR_HF.csv
    jobid: 3
    reason: Code has changed since last execution
    wildcards: trait=HF
    resources: tmpdir=/rds/user/jhz22/hpc-work/work

Some jobs were triggered by provenance information, see 'reason' section in the rule displays above.
If you prefer that only modification time is used to determine whether a job shall be executed, use the command line option '--rerun-triggers mtime' (also see --help).
If you are sure that a change for a certain output file (say, <outfile>) won't change the result (e.g. because you just changed the formatting of a script or environment definition), you can also wipe its metadata to skip such a trigger via 'snakemake --cleanup-metadata <outfile>'. 
Rules with provenance triggered jobs: aggregate_MR

RuleException in line 32 of /home/jhz22/pQTLtools/inst/snakemake/workflow/Snakefile:
NameError: The name 'trait' is unknown in this context. Did you mean 'wildcards.trait'?
