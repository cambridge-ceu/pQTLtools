\chapter{Case-control allelic association}

\section{Introduction}

Allelic association or haplotype analysis of a set of markers is commonly done
on samples of unrelated individuals from one or more populations (Hawley \&
Kidd 1995; Kidd et al.  2000; Schneider et al.  1997; Terwilliger \& Ott 1994;
Cox 1998; Zhao, Curtis \& Sham 2000).  These are also used to study
associations between a putative disease locus and marker(s) in samples of cases
and controls (Ott 1999; Zhao, Curtis \& Sham 2000).

When a single marker is involved, allele frequency differences between cases
and controls can be detected using a Pearson contingency table $\chi^2$ (Woolf
1955; Workman \& Niswander 1970; Sham \& Curtis 1995; Sasieni 1997; Hirotsu et
al.  2001) or a log-likelihood ratio statistic.  When more markers are
involved, it is more appropriate to use a likelihood framework based on
haplotype frequencies.  The likelihood of observing a given sample is expressed
as a function of unknown haplotype frequencies, to be maximised by numerical
method such as the EM (Expectation-Maximisation) algorithm.  This is called
gene counting since the EM steps only involve counting genes.  Some simple
scenarios including a small number of markers are illustrated in
Appendix~\ref{genecount}.  The general situation of a putative disease locus
and multiple multiallelic markers has been implemented in early version of {\bf
EH} (Xie \& Ott 1993; Terwilliger \& Ott 1994) and its successor {\bf fehp}
(Zhao \& Sham 2002).

For complex traits with uncertain disease model, it is natural to adapt
model-free statistics similar to those used in linkage analysis.  Assuming a
generalised single locus model, its parameters can be constrained to yield the
same population disease prevalence ($K$) as considered by Curtis \& Sham
(1995).  The parameters of the disease model are treated as nuisance parameters
in the likelihood function, and a likelihood ratio test is constructed in which
the likelihood under the null hypothesis is maximised over only the nuisance
parameters, while the likelihood under the alternative hypothesis is maximised
over both the nuisance parameters and the association parameters.  Similarly
the heterogeneity statistic in linkage analysis can be obtained from separate
marker-marker analyses of cases only, controls only and combined case-control
data.

It is then necessary to address issues concerning about validity and power of
such statistics.  In general, with its simple, well-known, practical and
economic advantages compared to others it is of great interest to fully
characterise and apply such design while avoiding spurious association (Long
\& Longley 1999; Risch \& Teng 1998; Devlin \& Roeder 1999; Risch 2000).  A
power analysis using the approach of Long et al.  (1997) will be given in
Chapter 7.


\subsection*{Chapter aims}

This chapter presents a power study of model-free tests in case-control
association analysis.  Similar study has been reported in Zhao \& Sham (1997),
Zhao, Curtis \& Sham (2000) but current investigation includes ordinary
contingency table $\chi^2$ and likelihood ratio tests.


\section{Methods}

Following Xie \& Ott (1993), we start by assuming a generalised single locus
model, in which a disease locus with one disease allele and one normal allele.
Under this model, the allele frequency of the disease allele $q$ and penetrance
$f_i$, $i=0,1,2$ for genotype with $i$ disease allele(s) are similarly defined
as in Chapter 2.  Contingency table Pearson $\chi^2$ (P) and LRT (G) statistics
are also used for comparison.  We consider also a haplotype model involving a
multiallelic marker B with $n$ ($n\ge 2$) alleles $B_1, B_2, \ldots, B_n$ and
allele frequencies $p_1, p_2,\ldots, p_n$.  The genotypic probabilities, based
on a subject being a case (A) or a control (U), are as follows,

\begin{eqnarray}
P(B_i B_i | A) &=& \frac{f_0 h_{1i}^2+f_1 (2h_{1i}h_{2i})+f_2 h_{2i}^2} {K}\cr
P(B_i B_j | A) &=& \frac{f_0 (2h_{1i}h_{1j})+f_1 (2h_{1i}h_{2j}
+2h_{1j}h_{2i})+f_2 (2h_{2i} h_{2j})} {K} \cr
P(B_i B_i | U) &=& \frac{s_0 h_{1i}^2+s_1 (2h_{1i}h_{2i})+s_2 h_{2i}^2} {1-K} \cr
P(B_i B_j | U) &=& \frac{s_0
(2h_{1i}h_{1j})+s_1(2h_{1i}h_{2j}+2h_{1j}h_{2i})+s_2(2h_{2i}h_{2j})}{1-K}
\label{gendist}
\end{eqnarray}
where $h_{11}, h_{12}, \ldots, h_{1n}, h_{21}, h_{22}, \ldots, h_{2n}$ are the
haplotype frequencies, and $s_i=1-f_i$.  The likelihood of the data is a
product of these probabilities, and the parameters of this likelihood function
are therefore the disease model parameters and the haplotype frequencies (Sham
1998, p159).  The maximum log-likelihood under both linkage equilibrium and
linkage disequilibrium are obtained from {\bf fehp} (Zhao \& Sham 2002).  Since
these constitute nested models, twice the difference in log-likelihood should
be asymptotically $\chi^2$ with $n-1$ degrees of freedom.  Four log-likelihood
ratio test (LRT) $\chi^2$ statistics, a heterogeneity statistic, contingency
table $\chi^2$ and log-likelihood ratio statistic are considered
(Table~\ref{brief}).

\begin{table}[h]\centering
\caption{Description of test statistics\label{brief}}
\begin{tabular}{llllllll}
\\
\hline
Statistics & Descriptions & $q$ && $f_0$&$f_1$&$f_2$\\
\hline
\multicolumn{2}{l}{Model-based}\\
T & True model & \multicolumn{4}{c}{$q$, $f_0, f_1, f_2$ are user specified}\\
R & Mendelian recessive model & $\sqrt{K}$&&0&0&1\\
D & Mendelian dominant model & $1-\sqrt{1-K}$&&0&1&1\\
F & Maximised over $f_1$ given $K$ &$q$ &&\multicolumn{4}{c}{$f_0=f_1$ or $f_1=f_2$}\\
  & (see description below)\\
\multicolumn{2}{l}{Model-free}\\
H & Heterogeneity statistic& \multicolumn{4}{c}{model not needed}\\
P & Pearson contingency table $\chi^2$&\multicolumn{4}{c}{model not needed}\\
G & LRT $\chi^2$ for contingency table&\multicolumn{4}{c}{model not needed}\\
\hline
\end{tabular}

\end{table}

These statistics are briefly described as follows.

{\bf T}:  This is the statistic one would obtain under a user-specified model,
which yields the correct population disease prevalence.

{\bf R}:  This is obtained with disease model parameters being set at Mendelian
recessive values ($q=\sqrt{K}$, $f_0=f_1=0$, $f_2=1$) in the case-control
option of {\bf fehp}.

{\bf D}:  This is obtained with disease model parameters being set at Mendelian
dominant values Mendelian penetrances ($1-\sqrt{1-K}$, $f_0=0$, $f_1=f_2=1$) in
the case-control option of {\bf fehp}.

{\bf F}:  This is obtained with disease model parameters being treated as
nuisance parameters.  The maximum likelihood is obtained from multiple runs of
{\bf fehp} with models that are constrained to produce the correct population
risk $K$ similar to {\bf MFLINK} (Curtis \& Sham 1995, see also Chapter 2),
which implies that only $f_1$ is free, and that $f_0 = f_1$, $f_2 =
1-f_1(1-K)/K$ when $f_1\leq K$, $f_2 = f_1$, $f_0 = (1-f_1)K/(1-K)$ when
$f_1>K$.  Specifically, $f_1$ is restricted to vary along the sides of the
triangle of $(0,0,1)$, $(K,K,K)$ and $(0, 1, 1)$.  Given $f_0$, $f_1$,
$f_2$, $q$ is obtained by solving $q^2f_2+2q(1-q)f_1+(1-q)^2f_0 = K$, namely
$$q=\frac{-2(f_1-f_0) + \sqrt{[2(f_1-f_0)]^2 - 4(f_0-2f_1+f_2)(f_0-K)}}
{2(f_0-2f_1+f_2)}$$ and $q=0.5(K-f_0)/(f_1-f_0)$ if $(f_0-2f_1+f_2)=0$ and 1 if
$f_2=K$.  Unlike {\bf MFLOD}, this statistic does not maximise $f_1$ in the
denominator of the likelihood ratio, the likelihood under the null is always
the likelihood assuming linkage equilibrium. To allow for an extra degree
of freedom maximising $f_1$, this should have $n$ degree of freedom.

{\bf H}:  This is a heterogeneity test in allele frequencies between cases and
controls.  As described above, the {\bf fehp} program is used three times, once
for cases alone, controls alone, and once for the cases and controls pooled
together.  In each analysis, allele frequencies are estimated and the maximum
log-likelihood calculated.  The likelihood of an individual is simply the
probability of the genotype assuming HWE.  Denoting these maximum
log-likelihoods as $l_{case}$, $l_{control}$ and $l_{combine}$, the test
statistics -$2(l_{case}+l_{control}-l_{combine}$) is asymptotically $\chi^2$
with $[(n-1) + (n-1) + (n-1)]=n-1$ degree of freedom.  The EM algorithm
requires at least two loci, but now only one marker is involved, so a
monomorphic marker is used for these individual marker-marker analyses.  This
statistic tests allele frequency differences between cases and controls
assuming Hardy-Weinberg equilibrium.

Statistics {\bf G} and {\bf P} are based on $2\times n$ contingency table in
this case; both are asymptotically $\chi^2$ with $n-1$ degrees of freedom.
Pearson $\chi^2$ statistic is more familiar while likelihood ratio test
statistic is more comparable to the model-based statistics above.  Note both G
and P use alleles rather than genotypes, so they require HWE to be valid tests
of association (Sasieni 1997).  P and G can also be based on the genotypes but
their degrees of freedom will be much larger for comparison with statistics
given above.  Hence subsequent calculations are based on allele counts.

Haplotype frequencies from Oudet et al.  (1993) on fragile X Syndrome are used
to evaluate power of model-free tests against correctly or incorrectly
specified parametric tests.  The frequencies of the seven alleles at DXS548 are
0, 42, 32, 1, 1, 29, 1 on fragile X chromosomes and 2, 117, 23, 1, 1, 15, 2 on
normal chromosomes.  The frequency of the disease allele is set arbitrarily to
be 0.001 for both R and D.  For each single gene disease models given
Table~\ref{model}, 500 replicate samples of 1,000 subjects (500 cases and 500
controls) and 500 replicate samples of 10,000 subjects (5,000 cases and 5,000
controls), are simulated in order to investigate the accuracy of the asymptotic
$\chi^2$ distribution as a function of sample size.

Five Mendelian models (Table~\ref{model}) with reduced penetrances and
phenocopies are used to simulate data in order to examine the performances of
test statistics.  They vary from simple Mendelian to recessive and dominant
models, both rare and common.  The choice of these models follows similar
argument in Chapter 2.  A minor gene model specifies a multiplicative model
with genotype relative risk of 4, as has been used earlier for Alzheimer's
(Sham \& Curtis 1995a, see also Chapter 2).

\begin{table}[h]\centering
\caption{The five genetic models\label{model}}
\begin{tabular}{lrrrcc}
\\
\hline
 Model                 & $f_0$ & $f_1$ & $f_2$ &  $q$     &   $K$\\
\hline
0 Null ($H_0$)         &   0.5 &  0.5  & 0.5   &0.5000    &0.500\\
1 Rare recessive (RR)  &     0 &    0  &   1   &0.0316    &0.001\\
2 Rare dominant (RD)   &     0 &    1  &   1   &0.0005    &0.001\\
3 Common recessive (CR)& 0.005 &0.005  & 0.5   &0.1000    &0.010\\
4 Common dominant (CD) & 0.005 &0.500  & 0.5   &0.0050    &0.010\\
5 Minor gene (MG)      & 0.050 &0.200  & 0.8   &0.1300    &0.100\\
\hline
\end{tabular}
\end{table}

The properties of each statistic under each model are investigated using
simulated samples.  The mean value of the test statistics of the replicates is
an estimator of its theoretical expectation (which is the sum of the
noncentrality parameter and degree of freedom).  This allows the noncentrality
parameter to be estimated.  Under the null hypothesis ($H_0$), the
noncentrality parameter should be 0.  A value greater than 0 implies an
increase in the false positive rate, while a value less than 0 indicates that
the test is conservative.  Under an alternative hypothesis, the non-centrality
parameter determines the power of the test.  At 5\% significance level, the
values of non-centrality parameter required for 90\% power are 17.4 and 18.3
for degrees of freedom 6 and 7, respectively.  The required sample size can be
extrapolated from the estimate of noncentrality parameter of the simulated
samples for any desired level of power.  Here the empirical means obtained from
simulated samples of size 10,000 are used as approximate estimates of
noncentrality parameters so the required samples sizes can be obtained (90\%
power at 5\% significance, assuming equal number of cases and controls).  An
exception is specifying null model ($H_0$) as in Table~\ref{model} would result
in zero log-likelihood.


\section{Results}

The results for 500 replicates of 500 cases and 500 controls are given in
Table~\ref{mf1}.  Under $H_0$, the empirical means exceed the theoretical means
(6) for R, D, and H, the reverse is true for F (7).  Interestingly, H and G are
very close to R.  While P and G compare fairly well, the mean of P as a
$\chi^2$ statistic is fairly close to its asymptotic value (6) but G tends to
be larger.  The likelihood-based statistics R, D, H are at least as good as
ordinary log-likelihood ratio statistic G, so their inflation under $H_0$ is
acceptable considering the wide use of G.  The statistics under the remaining
models provide estimates of noncentrality parameters of the associated $\chi^2$
distribtion, which can be used for obtaining sample size given certain
significance level and power (data not shown).  We expect that under
alternative hypotheses the empirical means from simulated samples of size
10,000 are approximately 10 times the corresponding values from that of size
1,000, as demonstrated below.

\begin{table}[h]\centering
\caption{Mean and standard deviation of $\chi^2$ statistics from
1,000 subjects\label{mf1}} (500 replicates of 500 cases and 500 controls)\\
\begin{tabular}{crrrrrrr}
\\
\hline
Model   &    T &      R &       D &        F &       H &       P &       G\\
\hline
$H_0$&         &   6.15 &    6.08 &     6.38 &    6.17 &    6.03 &    6.18\\%5.39
     &         &   3.33 &    3.22 &     3.42 &    3.37 &    3.22 &    3.38\\%2.80
RR   &  327.22 & 327.22 &  242.03 &   327.30 &  327.22 &  312.99 &  327.17\\
     &   33.96 &  33.96 &   23.16 &    33.98 &   33.96 &   31.83 &   33.92\\
RD   &  109.64 &  96.63 &  109.64 &   109.79 &   96.63 &   95.05 &   96.64\\
     &   19.16 &  18.02 &   19.16 &    19.23 &   18.02 &   17.46 &   18.03\\
CR   &   85.21 &  77.09 &   59.31 &    86.31 &   77.09 &   76.04 &   77.08\\
     &   18.62 &  17.87 &   13.34 &    18.81 &   17.87 &   17.40 &   17.87\\
CD   &   30.57 &  30.37 &   31.40 &    32.02 &   30.37 &   30.05 &   30.37\\
     &   10.17 &  10.21 &   10.32 &    10.43 &   10.20 &   10.04 &   10.20\\
MG   &   31.88 &  32.88 &   31.94 &    33.84 &   32.90 &   32.56 &   32.90\\
     &   10.88 &  11.14 &   10.73 &    11.30 &   11.13 &   10.98 &   11.14\\
\hline
\end{tabular}
\end{table}

The results for 500 replicates of 5,000 cases and 5,000 controls are given in
Table~\ref{mf2}.  Under $H_0$, the empirical means are slightly closer to their
theoretical values with the exception of F, which has an empirical mean of 6.24
while the theoretical mean is 7.  This indicates that the asymptotic
distribution of this statistic is closer to $\chi^2$ with 6 degrees of freedom
than $\chi^2$ with 7 degrees of freedom.  Again H and G are fairly close.

\begin{table}[h]\centering
\caption{Mean and standard deviation of $\chi^2$ statistics from
10,000 subjects\label{mf2}} (500 replicates of 5,000 cases and 5,000 controls)\\
\begin{tabular}{crrrrrrr}
\\
\hline
Model   &    T &      R &       D &        F &       H &       P &       G\\
\hline
$H_0$&         &   6.15 &    6.16 &     6.24 &    6.16 &    6.14 &    6.16\\%6.09
     &         &   3.55 &    3.56 &     3.60 &    3.55 &    3.53 &    3.55\\%3.48
RR   & 3199.29 &3199.29 & 2356.60 &  3199.29 & 3199.28 & 3063.71 & 3198.00\\
     &  111.16 & 111.11 &   77.51 &   111.11 &  111.12 &  104.43 &  110.95\\
RD   & 1041.95 & 907.50 & 1041.95 &  1041.95 &  907.50 &  895.77 &  907.54\\
     &   57.35 &  52.20 &   57.35 &    57.36 &   52.21 &   50.89 &   52.22\\
CR   &  804.95 & 716.72 &  541.28 &   805.37 &  716.69 &  710.20 &  716.74\\
     &   60.35 &  57.13 &   42.14 &    60.39 &   57.13 &   56.10 &   57.13\\
CD   &  261.98 & 252.31 &  262.59 &   262.73 &  252.31 &  251.13 &  252.35\\
     &   31.54 &  30.54 &   31.63 &    31.62 &   30.53 &   30.25 &   30.53\\
MG   &  268.79 & 270.35 &  261.75 &   270.98 &  270.23 &  269.16 &  270.26\\
     &   32.16 &  32.49 &   31.32 &    32.45 &   32.49 &   32.23 &   32.50\\
\hline
\end{tabular}
\end{table}

The required sample sizes based on 500 replicates of 5,000 cases and 5,000
controls are given in Table~\ref{mfn} (by a SAS program in Appendix).  It is
clear that rare disorders, RR in particular, require less subjects than common
disorders.  The number of subjects as required by CD is several folds that of
CR.  The number of subjects as required by major gene model is much larger than
that of rare disorder and somewhat between CR and CD considered here.

\begin{table}[h]\centering
\caption{Estimated sample sizes required for 90\% power and significance level
.05\label{mfn}} (equal number of cases and controls)\\
\begin{tabular}{crrrrrrrr}
\\
\hline
    Model&     T&      R&      D&      F&     H &   P &   G\\
\hline
     RR  &    55&     55&     74&     58&    55 &  57 &  55\\
     RD  &   167&    192&    167&    176&   192 & 195 & 192\\
     CR  &   217&    243&    322&    228&   243 & 246 & 243\\
     CD  &   665&    690&    663&    697&   690 & 693 & 690\\
     MG  &   648&    644&    665&    676&   644 & 647 & 644\\
\hline
\end{tabular}
\end{table}

No single test is uniformly more powerful over all 5 models.  T has the best
performance overall.  The power of R and H appear to be equivalent.  F is more
powerful than H in some situations.  For minor gene model, H appears to be
substantially more powerful than F.  If F is considered to have a $\chi^2$
distribution with 6 degrees of freedom, then F and H would be equally powerful
even in this situation, but then F would be slightly anti-conservative.

This shows that, when the disease model is unknown, the standard $\chi^2$ test
of homogeneity of allele frequencies (H) provides nearly optimal power,
especially for a minor gene model.  If the null distribution of the parametric
``model-free'' test (F) is accurately known, then this may be preferred to H.
Overall, the preferred test for routine use on case-control data is the
standard $\chi^2$ test of homogeneity of allele frequencies.  For data
involving related individuals, however, it is likely that parametric
``model-free'' methods will have a greater degree of superiority over
non-parametric methods.

A final observation is that for R and D models setting allele frequencies to be
0.001 other than obtaining from the correct population prevalence gives similar
results (data not shown).


\section{Discussion}

The performance of model-free statistics in case-control allelic association
analysis is investigated.  It is shown that the heterogeneity statistic is
almost as powerful as the likelihood ratio statistic taking true parameter
values of a generalised single locus model.

In view of the performance of the six statistics under the null hypothesis, the
power comparison is only considered as an approximation.  For instance,
standard errors of these statistics from Table 4 were about 0.05 ([standard
deviation]/$\sqrt{5000}$), the means of R, D, H are greater than 6, while that
of F is smaller than 7, the power calcualtion according to noncentral $\chi^2$
would be somewhat inflated for R, D, H but deflated for F, therefore the power
comparison may seem difficult.  However, similiar behaviour of the popular
statistic G offers some reassurance, as their relative performance, except F 
perhaps.  The limitaion of this study is its assumption of single locus disease 
models, which are unrealistic for complex diseases.  It further assumes the 
absence of hidden population stratification, a potential problem in real 
application.  While the study relies on asymptotic distribution theory, in 
practice Monte Carlo method should be used whenever appropriate (Zaykin et al. 
1995; Lazzeroni \& Lange 1997; Zhao, Curtis \& Sham 2000).

By including extra markers it is possible to construct a Pearson $\chi^2$
statistic and its likelihood ratio counterpart based on the haplotype table and
estimate a global LD statistic, but the Pearson $\chi^2$ statistic may often be
inflated and have poor asymptotic property.  It is common to construct such
statistics based on the genotypes of these markers.  Then test of independence
entails ``other association'' than that by the heterogeneity statistic
considered here.  These statistics for two markers has been implemented for two
markers in {\bf ASSOCIATE} (http://linkage.rockefeller.edu) and a more
comprehensive program {\bf 2LD} (Zapata et al.  2001).  The heterogeneity
statistic has been adopted both in the latest version of {\bf EH} and Fallin et
al.  (2001).  There would be concern over the validity of the chi-squared
statistics when the number of alleles become large, then empirical significance
should be used (Zhao, Curtis \& Sham 2000).  The concern over use of allelewise
versus genotypewise analysis has been extensively discussed by Sasieni (1997).

Both Koch et al.  (2000) and Fallin et al.  (2001) showed that an association
can be detected using either SNPs alone or combined with microsatellite markers
surrounding the functional locus even if the functional locus is not typed.  It
is remarkable that significant results were obtained for haplotypes defined by
loci that did not show single-locus significance.  Similar remark was made by
Longmate (2001).  It is likely that when loci influencing disease have alleles
whose impact is on the genotype level, tests using that fact may be more
powerful.  Moreover, the haplotype method does not necessarily help determine
the precise position of functional locus.  Similar note concerning fine genetic
mapping was given by Chiano \& Clayton (1998).  When parental phase is known it
can be directly used, otherwise the E-M algorithm can be applied.  While
haplotype analysis is invaluable for study of SNPs, where haplotypes will
increase the informativeness, there are theoretical and practical issues when
many loci are involved (e.g.  Zhao, Curtis \& Sham 2000).

A final note is that the case-control design should be valuable in homogeneous
or isolated populations (Shifman \& Darvasi 2001) and it is gaining more
attention (Devlin \& Roeder 1999; Risch 2000; Ghosh et al.  2000; Zhao et al.
2000; Seltman et al.  2001).


\section{Bibliographic notes}

Chapman \& Meng (1966), Guenther (1977) presented a simple formula to calculate
noncentrality parameter for alternative hypothesis of type
$h_{ij}=p_iq_j+c_{ij}/\sqrt{n}$, $\sum_{i=1}^m\sum_{j=1}^nc_{ij}=0$, i.e.,
$\lambda=\sum_{i=1}^m\sum_{j=1}^nc_{ij}^2/(p_iq_j)+\sum_{i=1}^m
c_{i.}^2/p_i+\sum_{j=1}^n c_{.j}^2/q_j$, where $c_{i.}=\sum_{j=1}^nc_{ij}$,
$c_{.j}=\sum_{i=1}^nc_{ij}$.  For two biallelic markers this is comparable to
$\lambda=2nD^2/(p_1p_2q_1q_2)$ (Weir 1996, p113), where $p$'s and $q$'s
are the allele frequencies, which is based on standard error of $D$.

Feder et al.  (1996) and Nielson et al. (1998) suggested examining departure
from HWE among affected individuals for fine-mapping of disease.  For recessive
$(\psi, \psi, 1)$ discrete model, the genotype and allele frequencies in the
population are expressed as $P(DD|A)=p_D^2/K$, $P(D|A)=p_D(p_D+\psi p_D)/K$, so
that the disequilibrium coefficient becomes
$D_{DD}=P_{DD}-p_D^2=[\psi(1-\psi)p_D^2(1-p_D^2)]/K^2$.  The quantity to
measure the departure from the Hardy-Weinberg equilibrium is
$F_D=(H_o-H_e)/(1-H_e)$ where $H_o$ and $H_e$ are the observed and expected
homozygosities, respectively.  It can be rewritten as
$F_D=[P_{DD|A}+P_{dd|A}-p_{D|A}^2-p_{d|A}^2]/(1-p_D^2-p_d^2)=[
\psi(1-\psi)p_Dp_d]/K^2$.  Association between the disease-susceptibility
allele D and a marker allele B can be expressed as $\Delta_{DB}=h_{DB}-p_Dq_B$,
where $h_{DB}$ is the frequency of haplotype carrying both alleles A and B and
$q_B$ is the frequency of marker allele $B$.  For Hardy-Weinberg
disequilibrium, $P_{BB|A}=[(1-\psi)(p_Dq_B+\Delta_{DB})^2+\psi q_B^2]/K$ and
$q_{B|A}=[\psi q_B+(1-\psi)p_D(p_Dq_B+\Delta_{DB})]/K$ so that the
Hardy-Weinberg disequilibrium coefficient among affected individuals is
$\Delta_{BB|A}=\psi(1-\psi)\Delta_{DB}^2/K^2$.  The Hardy-Weinberg departure of
Feder et al.  (1996) for the marker locus B is
$\psi(1-\psi)\Delta_{DB}^2/(K^2q_Bq_b)$ and $F_B=\Delta^2F_D$ where
$\Delta^2=\Delta_{DB}^2/p_Dp_dq_Bq_b$.  As $\Delta_{DB}$ is a function of
$(1-\theta)^{2g}$, where $g$ is the number of generations since the founding of
the mutation, $F_B$ decays approximately at a rate of $2g\theta$ with
recombination distance $\theta$ between $D$ and B.  With
$q_{B|U}=(1-\psi)[q_B-p_D(p_Dq_B+\Delta_{DB})]/(1-K)$, the commonly used LD
measure (Bengtsson and Thomson 1981, Lehesjoki et al.  1993),
$p_{excess}=(q_{B|A}-q_{B|U})/(1-q_{B|U})$ is
$$p_{excess}=\frac{(1-\psi)p_D\Delta_{DB}}{K(1-K)[q_B+(1-\psi)p_D\Delta_{DB}/(1
-K)]}$$ which is only proportional to $\Delta_{DB}$ and less sensitive than
$F_B$.  For a general disease model with multiallelic susceptibility locus and
a marker locus $B$ with $m$ allele, define $f_{rs}$ to be the ``apparent''
penetrance of genotype $D_rD_s$, so that $K=\sum_r\sum_sf_{rs}p_rp_s$, the
marker allele frequencies among the affecteds and unaffecteds are
$q_{i|A}=q_i+\delta_i/K$, $q_{i|U}=q_i-\delta_i/(1-K)$ and
$$p_{excess_i}=\frac{\delta_i}{K(1-K)[(1-q_i)+\delta_i/(1-K)]}$$ where
$\delta_i=\sum_r\sum_s p_s f_{rs}\Delta_{ri}$ and $\Delta_{ri}=h_{ri}-p_rq_i$.
The Hardy-Weinberg disequilibrium coefficients are
$\Delta_{ii|A}=h_{ii|A}-q_{i|A}^2=(K\delta_{ii}-\delta_i^2)/K^2$,
$\Delta_{ij|A}=h_{ij|A}-2q_{i|A}q_{j|A}=2(K\delta_{ij}-\delta_j\delta_j)/K^2$
and $\delta_{ij}=\sum_r\sum_s f_{rs}\Delta_{ri}\Delta_{sj}$.  A contingency
table $\chi^2$ can be built up using $n$ cases and $n$ controls as
$\chi_{CC}^2=4n\sum_i(\tilde p_{i|A}-\tilde p_{i|U})^2/(\tilde p_{i|A}+\tilde
p_{i|U})$, where $\tilde p_{i|A}$ and $\tilde p_{i|U}$ are the sample
frequencies of the marker, $\chi_{CC}^2$ has $\chi^2_{m-1}$ distribution under
the null hypothesis of no association and $\chi^2_{m-1}(\delta_{CC})$
distributed under the alternative, where
\begin{eqnarray*}
\delta_{CC}&=&4n\sum_i\frac{(q_{i|A}-q_{i|U})^2}{q_{i|A}+q_{i|U}}\cr
            &=&4n\sum_i\frac{\delta_i^2}{K^2(1-K)^2[2q_i+(1-2K)\delta_i/K(1-K)]}
\end{eqnarray*}
Similarly a Hardy-Weinberg disequilibrium test $\chi^2_{HW}$ with
$m(m+1)/2-(m-1)-1=m(m-1)/2$ degree(s) of freedom can be established.  The
noncentrality parameters for the two tests $\delta_{CC}$ and $\delta_{HW}$ are
given in Nielson et al (1998).  They noted that if the penetrances are regarded
as genotypic values, much of the theory can be applied to study of quantitative
traits (Nielsen and Weir 1999, 2001).

Zaykin et al.  (1995) describe simulation procedure to examine power of
haplotype analysis using unrelated individuals.  Mckeigue (2000) gave the
information loss in case-control design relative to family design.  Toivonen et
al.  (2000) discussed data mining.
